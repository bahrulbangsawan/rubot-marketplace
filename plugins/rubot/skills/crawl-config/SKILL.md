---
name: crawl-config
description: |
  Generate and configure crawl directives for search engines and AI crawlers. Use when setting up robots.txt, generating sitemap.xml from routes, configuring AI bot policies, or optimizing crawl budget.

  Covers: robots.txt templates, sitemap.xml generation, AI crawler directives, TanStack Start API routes, and Cloudflare Workers integration.
---

# Crawl Configuration Skill

> robots.txt and sitemap.xml generation

## When to Use

Use this skill when:
- Setting up robots.txt for a new site
- Generating sitemap.xml from routes
- Configuring crawl directives for AI bots
- Managing indexing for different environments
- Optimizing crawl budget

## robots.txt

### Basic Template

```
# robots.txt for https://example.com

User-agent: *
Allow: /
Disallow: /api/
Disallow: /admin/
Disallow: /auth/
Disallow: /_/
Disallow: /*.json$

# Sitemap location
Sitemap: https://example.com/sitemap.xml
```

### Comprehensive Template

```
# robots.txt for https://example.com
# Generated by rubot SEO

# ===================
# Default rules for all crawlers
# ===================
User-agent: *
Allow: /
Disallow: /api/
Disallow: /admin/
Disallow: /auth/
Disallow: /dashboard/
Disallow: /settings/
Disallow: /_/
Disallow: /*.json$
Disallow: /*?*  # Query parameters (optional)

# Crawl rate limiting (optional, not all bots respect this)
Crawl-delay: 1

# ===================
# Search Engine Bots
# ===================

# Google
User-agent: Googlebot
Allow: /
Disallow: /api/
Disallow: /admin/
Disallow: /auth/

User-agent: Googlebot-Image
Allow: /images/
Allow: /media/
Disallow: /

# Bing
User-agent: Bingbot
Allow: /
Disallow: /api/
Disallow: /admin/
Disallow: /auth/

# ===================
# AI Crawlers
# ===================

# OpenAI GPTBot
User-agent: GPTBot
Allow: /blog/
Allow: /docs/
Allow: /help/
Disallow: /

# OpenAI ChatGPT
User-agent: ChatGPT-User
Allow: /blog/
Allow: /docs/
Disallow: /

# Anthropic Claude
User-agent: anthropic-ai
Allow: /blog/
Allow: /docs/
Disallow: /

User-agent: Claude-Web
Allow: /blog/
Allow: /docs/
Disallow: /

# Google AI (Bard/Gemini)
User-agent: Google-Extended
Allow: /blog/
Allow: /docs/
Disallow: /

# Common Crawl
User-agent: CCBot
Allow: /blog/
Disallow: /

# ===================
# Block Bad Bots
# ===================
User-agent: AhrefsBot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /

# ===================
# Sitemaps
# ===================
Sitemap: https://example.com/sitemap.xml
Sitemap: https://example.com/sitemap-blog.xml
Sitemap: https://example.com/sitemap-products.xml
```

### Environment-Specific robots.txt

```
# Production (public)
User-agent: *
Allow: /

# Staging/Preview (block all)
User-agent: *
Disallow: /

# Development (block all)
User-agent: *
Disallow: /
```

### TanStack Start Implementation

```tsx
// src/routes/robots.txt.ts
import { createAPIFileRoute } from '@tanstack/start/api';

export const Route = createAPIFileRoute('/robots.txt')({
  GET: async () => {
    const isProduction = process.env.NODE_ENV === 'production';
    const baseUrl = process.env.SITE_URL || 'https://example.com';

    let content: string;

    if (isProduction) {
      content = `# robots.txt for ${baseUrl}

User-agent: *
Allow: /
Disallow: /api/
Disallow: /admin/
Disallow: /auth/
Disallow: /dashboard/
Disallow: /_/

# AI Crawlers
User-agent: GPTBot
User-agent: ChatGPT-User
User-agent: anthropic-ai
User-agent: Claude-Web
Allow: /blog/
Allow: /docs/
Disallow: /

Sitemap: ${baseUrl}/sitemap.xml
`;
    } else {
      content = `# robots.txt - Non-production environment
User-agent: *
Disallow: /
`;
    }

    return new Response(content, {
      headers: {
        'Content-Type': 'text/plain',
        'Cache-Control': 'public, max-age=3600',
      },
    });
  },
});
```

## sitemap.xml

### Basic Template

```xml
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
  <url>
    <loc>https://example.com/</loc>
    <lastmod>2024-01-15</lastmod>
    <changefreq>daily</changefreq>
    <priority>1.0</priority>
  </url>
  <url>
    <loc>https://example.com/about</loc>
    <lastmod>2024-01-10</lastmod>
    <changefreq>monthly</changefreq>
    <priority>0.8</priority>
  </url>
</urlset>
```

### Sitemap Index (for large sites)

```xml
<?xml version="1.0" encoding="UTF-8"?>
<sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
  <sitemap>
    <loc>https://example.com/sitemap-pages.xml</loc>
    <lastmod>2024-01-15</lastmod>
  </sitemap>
  <sitemap>
    <loc>https://example.com/sitemap-blog.xml</loc>
    <lastmod>2024-01-15</lastmod>
  </sitemap>
  <sitemap>
    <loc>https://example.com/sitemap-products.xml</loc>
    <lastmod>2024-01-14</lastmod>
  </sitemap>
</sitemapindex>
```

### Image Sitemap

```xml
<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
        xmlns:image="http://www.google.com/schemas/sitemap-image/1.1">
  <url>
    <loc>https://example.com/page</loc>
    <image:image>
      <image:loc>https://example.com/image.jpg</image:loc>
      <image:title>Image Title</image:title>
      <image:caption>Image caption description</image:caption>
    </image:image>
  </url>
</urlset>
```

### TanStack Start Implementation

```tsx
// src/routes/sitemap.xml.ts
import { createAPIFileRoute } from '@tanstack/start/api';
import { getAllPosts } from '~/lib/blog';
import { getAllProducts } from '~/lib/products';

interface SitemapUrl {
  loc: string;
  lastmod?: string;
  changefreq?: 'always' | 'hourly' | 'daily' | 'weekly' | 'monthly' | 'yearly' | 'never';
  priority?: number;
}

function generateSitemap(urls: SitemapUrl[], baseUrl: string): string {
  const urlElements = urls.map(url => `
  <url>
    <loc>${baseUrl}${url.loc}</loc>
    ${url.lastmod ? `<lastmod>${url.lastmod}</lastmod>` : ''}
    ${url.changefreq ? `<changefreq>${url.changefreq}</changefreq>` : ''}
    ${url.priority !== undefined ? `<priority>${url.priority.toFixed(1)}</priority>` : ''}
  </url>`).join('');

  return `<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
${urlElements}
</urlset>`;
}

export const Route = createAPIFileRoute('/sitemap.xml')({
  GET: async () => {
    const baseUrl = process.env.SITE_URL || 'https://example.com';

    // Static pages
    const staticPages: SitemapUrl[] = [
      { loc: '/', changefreq: 'daily', priority: 1.0 },
      { loc: '/about', changefreq: 'monthly', priority: 0.8 },
      { loc: '/contact', changefreq: 'monthly', priority: 0.7 },
      { loc: '/pricing', changefreq: 'weekly', priority: 0.9 },
      { loc: '/blog', changefreq: 'daily', priority: 0.9 },
    ];

    // Dynamic blog posts
    const posts = await getAllPosts();
    const blogUrls: SitemapUrl[] = posts.map(post => ({
      loc: `/blog/${post.slug}`,
      lastmod: post.updatedAt?.toISOString().split('T')[0] || post.publishedAt.toISOString().split('T')[0],
      changefreq: 'monthly' as const,
      priority: 0.7,
    }));

    // Dynamic products
    const products = await getAllProducts();
    const productUrls: SitemapUrl[] = products.map(product => ({
      loc: `/products/${product.slug}`,
      lastmod: product.updatedAt?.toISOString().split('T')[0],
      changefreq: 'weekly' as const,
      priority: 0.8,
    }));

    const allUrls = [...staticPages, ...blogUrls, ...productUrls];
    const sitemap = generateSitemap(allUrls, baseUrl);

    return new Response(sitemap, {
      headers: {
        'Content-Type': 'application/xml',
        'Cache-Control': 'public, max-age=3600',
      },
    });
  },
});
```

### Dynamic Route Discovery

```tsx
// src/lib/seo/sitemap.ts
import { routeTree } from '~/routeTree.gen';

export function discoverRoutes(): string[] {
  const routes: string[] = [];

  function traverse(node: any, path: string = '') {
    if (node.path) {
      const fullPath = path + node.path;
      // Skip dynamic routes and API routes
      if (!fullPath.includes('$') && !fullPath.includes('api')) {
        routes.push(fullPath);
      }
    }

    if (node.children) {
      Object.values(node.children).forEach((child: any) => {
        traverse(child, path + (node.path || ''));
      });
    }
  }

  traverse(routeTree);
  return routes;
}
```

## Priority Guidelines

| Page Type | Priority | Changefreq |
|-----------|----------|------------|
| Homepage | 1.0 | daily |
| Main categories | 0.9 | weekly |
| Product/service pages | 0.8 | weekly |
| Blog index | 0.9 | daily |
| Blog posts | 0.7 | monthly |
| About, Contact | 0.6 | monthly |
| Legal pages | 0.3 | yearly |

## Validation

### robots.txt Testing

```
// Check robots.txt accessibility
mcp__chrome-devtools__navigate_page({
  url: "https://example.com/robots.txt",
  type: "url"
})

// Take snapshot to verify content
mcp__chrome-devtools__take_snapshot()
```

### sitemap.xml Validation

```
// Check sitemap.xml
mcp__chrome-devtools__navigate_page({
  url: "https://example.com/sitemap.xml",
  type: "url"
})
```

## Cloudflare Workers Integration

```ts
// workers/sitemap.ts - Edge-generated sitemap
export default {
  async fetch(request: Request, env: Env): Promise<Response> {
    const url = new URL(request.url);

    if (url.pathname === '/sitemap.xml') {
      // Generate sitemap at edge
      const urls = await env.PAGES_KV.get('sitemap-urls', 'json');

      const sitemap = generateSitemap(urls, 'https://example.com');

      return new Response(sitemap, {
        headers: {
          'Content-Type': 'application/xml',
          'Cache-Control': 'public, max-age=3600, s-maxage=86400',
        },
      });
    }

    return fetch(request);
  },
};
```

## Best Practices

### robots.txt

1. Always include Sitemap directive
2. Use HTTPS in sitemap URL
3. Don't block CSS/JS needed for rendering
4. Test with Google Search Console
5. Keep directives simple and clear

### sitemap.xml

1. Maximum 50,000 URLs per sitemap
2. Maximum 50MB uncompressed
3. Use sitemap index for large sites
4. Include only canonical URLs
5. Update lastmod only when content changes
6. Don't include noindex pages

## References

- Google robots.txt: https://developers.google.com/search/docs/crawling-indexing/robots/intro
- Google Sitemaps: https://developers.google.com/search/docs/crawling-indexing/sitemaps/overview
- Sitemaps Protocol: https://www.sitemaps.org/protocol.html
- AI Crawler Guidelines: https://platform.openai.com/docs/gptbot
