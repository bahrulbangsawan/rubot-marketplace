---
name: seo-generate-robots
description: Generate robots.txt file for the project
---

# SEO Generate Robots Command

Generate a robots.txt file based on project requirements and environment.

## Execution Steps

### Step 1: Determine Project Type

Use AskUserQuestion to understand the project:

```
questions:
  - question: "What type of application is this?"
    header: "App Type"
    options:
      - label: "Public website"
        description: "Blog, marketing site, e-commerce (should be indexed)"
      - label: "Dashboard/Admin"
        description: "Internal tool, admin panel (should NOT be indexed)"
      - label: "SaaS application"
        description: "Mix of public and private pages"
      - label: "Documentation site"
        description: "Public docs (should be indexed)"
    multiSelect: false
```

### Step 2: Confirm AI Crawler Policy

```
questions:
  - question: "How should AI crawlers (GPTBot, ClaudeBot) be handled?"
    header: "AI Crawlers"
    options:
      - label: "Allow public content"
        description: "Let AI crawlers access blog/docs but not app"
      - label: "Allow all"
        description: "No restrictions for AI crawlers"
      - label: "Block all"
        description: "Prevent AI crawlers from accessing any content"
    multiSelect: false
```

### Step 3: Get Site URL

```
questions:
  - question: "What is your production site URL?"
    header: "Site URL"
    options:
      - label: "Enter URL"
        description: "I'll provide the production URL"
    multiSelect: false
```

### Step 4: Identify Paths to Block

Use Glob to find routes and identify protected paths:

```
Glob pattern: "src/routes/**/*.tsx"
```

Look for:
- `/api/*` - API routes
- `/admin/*` - Admin pages
- `/dashboard/*` - Dashboard pages
- `/auth/*` - Authentication pages
- `/settings/*` - User settings
- `/_/*` - Internal routes

### Step 5: Generate robots.txt

Based on project type, generate appropriate content:

#### Public Website Template

```
# robots.txt for [SITE_URL]
# Generated by rubot SEO

User-agent: *
Allow: /
Disallow: /api/
Disallow: /auth/
Disallow: /_/
Disallow: /*.json$

# Search Engine Crawlers
User-agent: Googlebot
Allow: /

User-agent: Bingbot
Allow: /

# AI Crawlers - Allow public content
User-agent: GPTBot
User-agent: ChatGPT-User
User-agent: anthropic-ai
User-agent: Claude-Web
User-agent: Google-Extended
Allow: /blog/
Allow: /docs/
Allow: /help/
Disallow: /

# Block aggressive crawlers
User-agent: AhrefsBot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: MJ12bot
Disallow: /

# Sitemap
Sitemap: [SITE_URL]/sitemap.xml
```

#### Dashboard/Admin Template

```
# robots.txt for [SITE_URL]
# Generated by rubot SEO
# This is a private application - all crawling is blocked

User-agent: *
Disallow: /

# Explicitly block all AI crawlers
User-agent: GPTBot
User-agent: ChatGPT-User
User-agent: anthropic-ai
User-agent: Claude-Web
User-agent: Google-Extended
User-agent: CCBot
Disallow: /
```

#### SaaS Application Template

```
# robots.txt for [SITE_URL]
# Generated by rubot SEO

# Default: Block app routes
User-agent: *
Allow: /
Allow: /pricing
Allow: /features
Allow: /about
Allow: /contact
Allow: /blog/
Allow: /docs/
Disallow: /app/
Disallow: /dashboard/
Disallow: /settings/
Disallow: /api/
Disallow: /auth/
Disallow: /_/

# AI Crawlers - Public content only
User-agent: GPTBot
User-agent: ChatGPT-User
User-agent: anthropic-ai
User-agent: Claude-Web
Allow: /blog/
Allow: /docs/
Disallow: /

# Sitemap
Sitemap: [SITE_URL]/sitemap.xml
```

### Step 6: Write File

Determine file location:

**For TanStack Start (API route)**:
```
// Create src/routes/robots.txt.ts
```

**For static hosting**:
```
// Create public/robots.txt
```

Use AskUserQuestion to confirm:

```
questions:
  - question: "How should robots.txt be served?"
    header: "Serving Method"
    options:
      - label: "Dynamic (API route)"
        description: "Generate via TanStack Start API route"
      - label: "Static file"
        description: "Create static public/robots.txt"
    multiSelect: false
```

#### Dynamic API Route

```tsx
// src/routes/robots.txt.ts
import { createAPIFileRoute } from '@tanstack/start/api';

export const Route = createAPIFileRoute('/robots.txt')({
  GET: async () => {
    const isProduction = process.env.NODE_ENV === 'production';
    const siteUrl = process.env.SITE_URL || 'https://example.com';

    const content = isProduction
      ? `# robots.txt for ${siteUrl}

User-agent: *
Allow: /
Disallow: /api/
Disallow: /auth/
Disallow: /dashboard/
Disallow: /_/

User-agent: GPTBot
User-agent: ChatGPT-User
User-agent: anthropic-ai
Allow: /blog/
Allow: /docs/
Disallow: /

Sitemap: ${siteUrl}/sitemap.xml
`
      : `# Non-production - block all
User-agent: *
Disallow: /
`;

    return new Response(content, {
      headers: {
        'Content-Type': 'text/plain',
        'Cache-Control': 'public, max-age=3600',
      },
    });
  },
});
```

#### Static File

Write content to `public/robots.txt`.

### Step 7: Verify Creation

After writing:

1. Run dev server
2. Navigate to /robots.txt
3. Verify content is correct

```
mcp__chrome-devtools__navigate_page({
  url: "http://localhost:3000/robots.txt",
  type: "url"
})

mcp__chrome-devtools__take_snapshot()
```

### Step 8: Provide Validation Instructions

```markdown
## robots.txt Created Successfully

**Location**: [file path]
**Type**: [Dynamic/Static]

### Verification Steps

1. Start dev server: `bun run dev`
2. Visit: http://localhost:3000/robots.txt
3. Verify content matches expected output

### Production Verification

After deployment:
1. Visit: [SITE_URL]/robots.txt
2. Test with Google's robots.txt Tester in Search Console

### Environment Behavior

| Environment | Behavior |
|-------------|----------|
| Development | Blocks all crawlers |
| Staging | Blocks all crawlers |
| Production | Applies defined rules |

### Next Steps

1. Run `/seo-generate-sitemap` to create sitemap.xml
2. Submit sitemap to Google Search Console
3. Test with `/seo-audit` command
```

## Common Directives Reference

| Directive | Purpose |
|-----------|---------|
| `User-agent: *` | Apply rules to all crawlers |
| `Allow: /` | Allow crawling of path |
| `Disallow: /` | Block crawling of path |
| `Crawl-delay: 1` | Rate limit (seconds between requests) |
| `Sitemap: URL` | Location of sitemap |

## AI Crawler User-Agents

| User-Agent | Company |
|------------|---------|
| GPTBot | OpenAI |
| ChatGPT-User | OpenAI |
| anthropic-ai | Anthropic |
| Claude-Web | Anthropic |
| Google-Extended | Google (Bard/Gemini) |
| CCBot | Common Crawl |

## Related Commands

- `/seo-generate-sitemap` - Generate sitemap.xml
- `/seo-audit` - Full SEO audit

## Related Skills

- `crawl-config` - robots.txt and sitemap patterns
